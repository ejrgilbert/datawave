CONFIGURATION=compose
RCPT_TO=hadoop@localhost

# ingest properties
DATAWAVE_INGEST_HOME=/opt/datawave-ingest/current

WAREHOUSE_ACCUMULO_HOME=/opt/accumulo/current
WAREHOUSE_HDFS_NAME_NODE=hdfs://namenode:8020
WAREHOUSE_JOBTRACKER_NODE=resourcemanager:8032
WAREHOUSE_ZOOKEEPERS=zoo1:2181,zoo2:2181,zoo3:2181
WAREHOUSE_INSTANCE_NAME=COMPOSE
#Sets variable sets the zookeeper location for the warehouse side
zookeeper.hosts=zoo1:2181,zoo2:2181,zoo3:2181

INGEST_ACCUMULO_HOME=/opt/accumulo/current
INGEST_HDFS_NAME_NODE=hdfs://namenode:8020
INGEST_JOBTRACKER_NODE=resourcemanager:8032
INGEST_ZOOKEEPERS=zoo1:2181,zoo2:2181,zoo3:2181
INGEST_INSTANCE_NAME=COMPOSE

JOB_CACHE_REPLICATION=1

STAGING_HOSTS=`nodeattr -c staging`
DUMPER_HOSTS=ingestmaster
INGEST_HOST=ingestmaster
ROLLUP_HOST=ingestmaster

#extra mapreduce options (e.g. mapreduce.task.io.sort.mb and the like)
MAPRED_INGEST_OPTS=-compressionType gz -useInlineCombiner -dfs.replication=2 -tableCounters -contextWriterCounters -markerFileLIFO

#extra HADOOP_OPTS (java options)
HADOOP_INGEST_OPTS=

#extra CHILD_OPTS (java options)
CHILD_INGEST_OPTS=

BULK_CHILD_MAP_MAX_MEMORY_MB=2048
LIVE_CHILD_MAP_MAX_MEMORY_MB=1024
BULK_CHILD_REDUCE_MAX_MEMORY_MB=2048
LIVE_CHILD_REDUCE_MAX_MEMORY_MB=1024

BULK_INGEST_DATA_TYPES=shardStats
LIVE_INGEST_DATA_TYPES=wikipedia,mycsv,myjson

# Clear out these values if you do not want standard shard ingest.
DEFAULT_SHARD_HANDLER_CLASSES=datawave.ingest.mapreduce.handler.shard.AbstractColumnBasedHandler
ALL_HANDLER_CLASSES=datawave.ingest.mapreduce.handler.edge.ProtobufEdgeDataTypeHandler,datawave.ingest.mapreduce.handler.dateindex.DateIndexDataTypeHandler

BULK_INGEST_REDUCERS=10
LIVE_INGEST_REDUCERS=10

# Note the max blocks per job must be less than or equal to the number of mappers
INGEST_BULK_JOBS=1
INGEST_BULK_MAPPERS=4
INGEST_MAX_BULK_BLOCKS_PER_JOB=4
INGEST_LIVE_JOBS=1
INGEST_LIVE_MAPPERS=4
INGEST_MAX_LIVE_BLOCKS_PER_JOB=4

INDEX_STATS_MAX_MAPPERS=1

NUM_MAP_LOADERS=1

USERNAME=datawave
PASSWORD=secret

ZOOKEEPER_HOME=/usr/lib/zookeeper
HADOOP_HOME=/usr/lib/hadoop
MAPRED_HOME=/usr/lib/hadoop-mapreduce

WAREHOUSE_HADOOP_CONF=/etc/hadoop/conf
INGEST_HADOOP_CONF=/etc/hadoop/conf

HDFS_BASE_DIR=/data

MONITOR_SERVER_HOST=monitor

LOG_DIR=/srv/logs/ingest
PDSH_LOG_DIR=${LOG_DIR}/pdsh_logs
FLAG_DIR=/srv/data/datawave/flags
FLAG_MAKER_CONFIG=/opt/datawave-ingest/current/config/flag-maker-live.xml,/opt/datawave-ingest/current/config/flag-maker-bulk.xml
BIN_DIR_FOR_FLAGS=/opt/datawave-ingest/current/bin

PYTHON=/usr/bin/python

# Setting discard interval to 0 in order to disable auto-ageoff @ ingest time
EVENT_DISCARD_INTERVAL=0

DATAWAVE_CACHE_PORT=20444

EDGE_DEFINITION_FILE=config/edge-definitions.xml

ERROR_TABLE=errors
ANALYTIC_MTX=analytic_metrics
LOADER_MTX=loader_metrics
INGEST_MTX=ingest_metrics
BULK_INGEST_METRIC_THRESHOLD=1500000
LIVE_INGEST_METRIC_THRESHOLD=1500000

KEYSTORE=/etc/pki/private/developerKey.p12
KEYSTORE_TYPE=PKCS12
KEYSTORE_PASSWORD=secret
TRUSTSTORE=/etc/pki/private/developerTruststore.jks

FLAG_METRICS_DIR=/srv/data/datawave/flagMetrics
TRUSTSTORE_PASSWORD=Changeit1
TRUSTSTORE_TYPE=JKS

cluster.name=WAREHOUSE
accumulo.instance.name=COMPOSE
accumulo.user.name=datawave
accumulo.user.password=secret
cached.results.hdfs.uri=hdfs://namenode:8020/
cached.results.export.dir=/CachedResults

lock.file.dir=/var/run/datawave
JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk/

# query properties
server.keystore.password=secret
mysql.user.password=datawave
jboss.jmx.password=blah
hornetq.cluster.password=blah
hornetq.system.password=blah

server.truststore.password=Changeit

#Sets up the Atom Service
atom.wildfly.hostname=localhost
atom.wildfly.port.number=8443
atom.connection.pool.name=WAREHOUSE


security.use.testauthservice=true
security.testauthservice.context.entry=<value>classpath*:datawave/security/TestDatawaveUserServiceConfiguration.xml</value>
security.testauthservice.users= \
\n        <value><![CDATA[ \
\n        { \
\n            "dn": { \
\n                "subjectDN": "cn=test a. user, ou=my department, o=my company, st=some-state, c=us", \
\n                "issuerDN": "cn=test ca, ou=my department, o=my company, st=some-state, c=us" \
\n            }, \
\n            "userType": "USER",\
\n            "auths": [ "PVT", "PUB" ], \
\n            "roles": [ "PRIVATE", "PUBLIC", "Administrator", "AuthorizedUser", "JBossAdministrator" ], \
\n            "roleToAuthMapping": { \
\n                "PRIVATE": [ "PVT" ], \
\n                "PUBLIC": [ "PUB" ] \
\n            }, \
\n            "creationTime": -1, \
\n            "expirationTime": -1 \
\n        } \
\n        ]]></value> \
\n        <value><![CDATA[ \
\n        { \
\n            "dn": { \
\n                "subjectDN": "cn=testserver.example.com, ou=d009, o=my company, st=some-state, c=us", \
\n                "issuerDN": "cn=test ca, ou=my department, o=my company, st=some-state, c=us" \
\n            }, \
\n            "userType": "SERVER",\
\n            "auths": [ "PVT", "PUB" ], \
\n            "roles": [ "PRIVATE", "PUBLIC", "AuthorizedServer" ], \
\n            "roleToAuthMapping": { \
\n                "PRIVATE": [ "PVT" ], \
\n                "PUBLIC": [ "PUB" ] \
\n            }, \
\n            "creationTime": -1, \
\n            "expirationTime": -1 \
\n        } \
\n        ]]></value>

# other properties
rpm.file.owner=datawave
rpm.file.group=datawave
rpm.file.accumulo.owner=accumulo
rpm.file.accumulo.group=accumulo

# Enable full table scans for the base event query?
#beq.fullTableScanEnabled=true

event.query.data.decorators= \
          <entry key="CSV"> \
\n            <bean class="datawave.query.transformer.EventQueryDataDecorator"> \
\n                <property name="fieldName" value="CSV"/> \
\n                <property name="patternMap"> \
\n                    <map key-type="java.lang.String" value-type="java.lang.String"> \
\n                        <entry key="EVENT_ID" value="https://localhost:8443/DataWave/Query/lookupUUID/EVENT_ID?uuid=@field_value@&amp;parameters=data.decorators:CSV"/> \
\n                        <entry key="UUID" value="https://localhost:8443/DataWave/Query/lookupUUID/UUID?uuid=@field_value@&amp;parameters=data.decorators:CSV"/> \
\n                        <entry key="PARENT_UUID" value="https://localhost:8443/DataWave/Query/lookupUUID/PARENT_UUID?uuid=@field_value@&amp;parameters=data.decorators:CSV"/> \
\n                    </map> \
\n                </property> \
\n            </bean> \
\n        </entry> \
\n        <entry key="WIKIPEDIA"> \
\n            <bean class="datawave.query.transformer.EventQueryDataDecorator"> \
\n                <property name="fieldName" value="WIKIPEDIA"/> \
\n                <property name="patternMap"> \
\n                    <map key-type="java.lang.String" value-type="java.lang.String"> \
\n                        <entry key="PAGE_ID" value="https://localhost:8443/DataWave/Query/lookupUUID/PAGE_ID?uuid=@field_value@&amp;parameters=data.decorators:WIKIPEDIA"/> \
\n                        <entry key="PAGE_TITLE" value="https://localhost:8443/DataWave/Query/lookupUUID/PAGE_TITLE?uuid=@field_value@&amp;parameters=data.decorators:WIKIPEDIA"/> \
\n                    </map> \
\n                </property> \
\n            </bean> \
\n        </entry>

lookup.uuid.uuidTypes= \
          <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="EVENT_ID" /> \
\n            <property name="definedView" value="LuceneUUIDEventQuery" /> \
\n            <property name="allowWildcardAfter" value="28" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="UUID" /> \
\n            <property name="definedView" value="LuceneUUIDEventQuery" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PARENT_UUID" /> \
\n            <property name="definedView" value="LuceneUUIDEventQuery" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PAGE_ID" /> \
\n            <property name="definedView" value="LuceneUUIDEventQuery" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PAGE_TITLE" /> \
\n            <property name="definedView" value="LuceneUUIDEventQuery" /> \
\n        </bean>

query.metrics.marking=(PUBLIC)
query.metrics.visibility=PUBLIC

metrics.warehouse.namenode=localhost
metrics.warehouse.hadoop.path=/usr/lib/hadoop
metrics.reporter.class=datawave.metrics.NoOpMetricsReporterFactory

metadatahelper.default.auths=PUBLIC

security.npe.ou.entries=EXAMPLE_SERVER_OU1,EXAMPLE_SERVER_OU2
security.subject.dn.pattern=(?:^|,)\\s*OU\\s*=\\s*My Department\\s*(?:,|$)

datawave.docs.menu.extras=<li><a href="http://localhost:9995">Accumulo</a></li>

